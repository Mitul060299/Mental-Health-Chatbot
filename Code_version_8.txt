{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import psutil\n",
    "import gc\n",
    "import time\n",
    "import requests\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import streamlit as st\n",
    "\n",
    "# Install required packages\n",
    "%pip install rouge_score bitsandbytes peft accelerate streamlit pyngrok -q\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Set up ngrok for Streamlit tunnel\n",
    "from pyngrok import ngrok\n",
    "!ngrok authtoken YOUR_NGROK_AUTH_TOKEN  # Replace with your ngrok token\n",
    "\n",
    "# Load and merge datasets\n",
    "def load_and_merge_datasets():\n",
    "    hope_path = \"/content/SPARTA_WSDM2022/HOPE_data/HOPE_therapy_session_transcripts\"\n",
    "    try:\n",
    "        files = [f for f in os.listdir(hope_path) if f.endswith(\".csv\")]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: HOPE dataset directory not found at {hope_path}\")\n",
    "        files = []\n",
    "\n",
    "    hope_pairs = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(hope_path, file))\n",
    "        df['Speaker'] = df['Type'].map({'T': 'Therapist', 'P': 'Client'})\n",
    "        df['Content'] = df['Utterance']\n",
    "        for i in range(1, len(df)):\n",
    "            if df.loc[i-1, 'Speaker'] == \"Client\" and df.loc[i, 'Speaker'] == \"Therapist\":\n",
    "                hope_pairs.append({\n",
    "                    \"prompt\": f\"Client: {df.loc[i-1, 'Content']}\",\n",
    "                    \"response\": f\"Therapist: {df.loc[i, 'Content']}\",\n",
    "                    \"source\": \"HOPE\"\n",
    "                })\n",
    "    print(f\"Extracted {len(hope_pairs)} dialogue pairs from HOPE dataset\")\n",
    "\n",
    "    try:\n",
    "        empathetic_ds = load_dataset(\"empathetic_dialogues\")\n",
    "        empathy_pairs = []\n",
    "        prev_conv_id = None\n",
    "        context = \"\"\n",
    "        for row in empathetic_ds['train']:\n",
    "            if row['utterance_idx'] > 0 and row['conv_id'] == prev_conv_id:\n",
    "                empathy_pairs.append({\n",
    "                    \"prompt\": f\"Client: {context}\",\n",
    "                    \"response\": f\"Therapist: {row['utterance']}\",\n",
    "                    \"emotion\": row['context'],\n",
    "                    \"source\": \"EmpatheticDialogues\"\n",
    "                })\n",
    "            context = row['utterance']\n",
    "            prev_conv_id = row['conv_id']\n",
    "        print(f\"Extracted {len(empathy_pairs)} dialogue pairs from EmpatheticDialogues dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading EmpatheticDialogues: {e}\")\n",
    "        empathy_pairs = []\n",
    "\n",
    "    try:\n",
    "        url = \"https://raw.githubusercontent.com/nbertagnolli/counsel-chat/master/data/counselchat-data.csv\"\n",
    "        response = requests.get(url)\n",
    "        with open(\"counselchat-data.csv\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        cc_df = pd.read_csv(\"counselchat-data.csv\")\n",
    "        counsel_pairs = []\n",
    "        for _, row in cc_df.iterrows():\n",
    "            if pd.notnull(row['questionText']) and pd.notnull(row['answerText']):\n",
    "                counsel_pairs.append({\n",
    "                    \"prompt\": f\"Client: {row['questionText']}\",\n",
    "                    \"response\": f\"Therapist: {row['answerText'].replace('<p>','').replace('</p>','').strip()}\",\n",
    "                    \"source\": \"CounselChat\"\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CounselChat: {e}\")\n",
    "        counsel_pairs = []\n",
    "\n",
    "    dialogue_data = hope_pairs + empathy_pairs + counsel_pairs\n",
    "    print(f\"Total dialogue pairs: {len(dialogue_data)}\")\n",
    "\n",
    "    print(\"\\nSample data from each source:\")\n",
    "    for source in [\"HOPE\", \"EmpatheticDialogues\", \"CounselChat\"]:\n",
    "        samples = [d for d in dialogue_data if d.get(\"source\") == source]\n",
    "        if samples:\n",
    "            print(f\"\\n{source} sample:\")\n",
    "            sample = np.random.choice(samples)\n",
    "            print(f\"Prompt: {sample['prompt']}\")\n",
    "            print(f\"Response: {sample['response']}\")\n",
    "\n",
    "    dialogue_data = [{'prompt': d['prompt'], 'response': d['response'], 'emotion': d.get('emotion', 'unknown')} for d in dialogue_data]\n",
    "    return Dataset.from_list(dialogue_data)\n",
    "\n",
    "# Load and split dataset\n",
    "dialogue_data = load_and_merge_datasets()\n",
    "train_data, val_data = train_test_split(dialogue_data.to_pandas(), test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_data.reset_index(drop=True))\n",
    "\n",
    "# Load Mistral 7B with 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "try:\n",
    "    bert_model = AutoModelForSequenceClassification.from_pretrained(\"bhadresh-savani/bert-base-uncased-emotion\")\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\"bhadresh-savani/bert-base-uncased-emotion\")\n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", quantization_config=quantization_config, device_map=\"auto\")\n",
    "    mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "    mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Fine-tune Mistral 7B with LoRA\n",
    "def tokenize_function(examples):\n",
    "    return mistral_tokenizer(examples['prompt'] + ' ' + examples['response'], truncation=True, max_length=512, padding='max_length')\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "mistral_model = get_peft_model(mistral_model, lora_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=mistral_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=mistral_tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "mistral_model.save_pretrained(\"./mistral-finetuned\")\n",
    "mistral_tokenizer.save_pretrained(\"./mistral-finetuned\")\n",
    "\n",
    "# Reload fine-tuned Mistral\n",
    "try:\n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(\"./mistral-finetuned\", quantization_config=quantization_config, device_map=\"auto\")\n",
    "    mistral_tokenizer = AutoTokenizer.from_pretrained(\"./mistral-finetuned\")\n",
    "    mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "except FileNotFoundError:\n",
    "    print(\"Fine-tuned Mistral not found. Using pre-trained model.\")\n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", quantization_config=quantization_config, device_map=\"auto\")\n",
    "    mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "    mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "\n",
    "# Define MentalHealthChatbot class\n",
    "class MentalHealthChatbot:\n",
    "    def __init__(self, bert_model, bert_tokenizer, mistral_model, mistral_tokenizer):\n",
    "        self.bert_model = bert_model\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.mistral_model = mistral_model\n",
    "        self.mistral_tokenizer = mistral_tokenizer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.bert_model.to(self.device)\n",
    "        self.mistral_model.eval()\n",
    "        self.feedback_buffer = []\n",
    "        self.history = []  # Store conversation history\n",
    "        self.user_profile = {'tone': 'neutral'}  # Default user profile\n",
    "\n",
    "    def set_user_profile(self, tone):\n",
    "        self.user_profile['tone'] = tone\n",
    "\n",
    "    def analyze_emotion(self, text):\n",
    "        inputs = self.bert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(**inputs)\n",
    "            emotion_scores = outputs.logits[0]\n",
    "            emotion_id = torch.argmax(emotion_scores).item()\n",
    "            emotion = self.bert_model.config.id2label[emotion_id]\n",
    "            confidence = torch.softmax(emotion_scores, dim=0)[emotion_id].item()\n",
    "        return emotion, confidence\n",
    "\n",
    "    def generate_response(self, user_input, max_length=50):\n",
    "        # Check for crisis keywords\n",
    "        crisis_keywords = [\"suicide\", \"self-harm\", \"kill\", \"die\", \"end my life\"]\n",
    "        if any(keyword in user_input.lower() for keyword in crisis_keywords):\n",
    "            return {\n",
    "                \"response\": \"I'm here to listen, but this sounds serious. Please contact a professional or call the 988 Suicide & Crisis Lifeline immediately.\",\n",
    "                \"emotion_detected\": \"crisis\",\n",
    "                \"confidence\": 1.0,\n",
    "                \"prompt\": user_input,\n",
    "                \"full_response\": \"\"\n",
    "            }\n",
    "\n",
    "        # Analyze emotion\n",
    "        emotion, confidence = self.analyze_emotion(user_input)\n",
    "\n",
    "        # Adjust temperature based on emotion\n",
    "        emotion_temperature = {\n",
    "            \"sad\": 0.3,\n",
    "            \"anger\": 0.4,\n",
    "            \"fear\": 0.3,\n",
    "            \"joy\": 0.7,\n",
    "            \"love\": 0.6,\n",
    "            \"surprise\": 0.5\n",
    "        }\n",
    "        temperature = emotion_temperature.get(emotion, np.random.uniform(0.3, 0.7))\n",
    "        top_p = np.random.uniform(0.8, 0.95)\n",
    "\n",
    "        # Build prompt with history, user profile, and emotion\n",
    "        history_context = \"\\n\".join(self.history[-3:]) + \"\\n\" if self.history else \"\"\n",
    "        prompt = f\"User tone: {self.user_profile['tone']}\\nEmotion: {emotion}\\n{history_context}Client: {user_input}\\nTherapist:\"\n",
    "        inputs = self.mistral_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        try:\n",
    "            output_sequences = self.mistral_model.generate(\n",
    "                **inputs,\n",
    "                max_length=len(inputs[\"input_ids\"][0]) + max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.mistral_tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.5\n",
    "            )\n",
    "            full_text = self.mistral_tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "            therapist_response = full_text.split(\"Therapist:\", 1)[1].strip() if \"Therapist:\" in full_text else full_text\n",
    "            if len(therapist_response.split()) > 100:\n",
    "                therapist_response = ' '.join(therapist_response.split()[:100])\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            therapist_response = \"I'm here to help. Could you share more?\"\n",
    "\n",
    "        # Update history\n",
    "        self.history.append(f\"Client: {user_input}\")\n",
    "        self.history.append(f\"Therapist: {therapist_response}\")\n",
    "\n",
    "        return {\n",
    "            \"response\": therapist_response,\n",
    "            \"emotion_detected\": emotion,\n",
    "            \"confidence\": confidence,\n",
    "            \"prompt\": prompt,\n",
    "            \"full_response\": full_text\n",
    "        }\n",
    "\n",
    "    def update_model_with_feedback(self):\n",
    "        if len(self.feedback_buffer) < 10:\n",
    "            return\n",
    "\n",
    "        feedback_data = [{'prompt': fb['prompt'], 'response': fb['response']} for fb in self.feedback_buffer if fb['rating'] == 1]\n",
    "        if not feedback_data:\n",
    "            self.feedback_buffer = []\n",
    "            return\n",
    "\n",
    "        feedback_dataset = Dataset.from_list(feedback_data)\n",
    "        feedback_dataset = feedback_dataset.map(tokenize_function, batched=True)\n",
    "        feedback_dataset.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "        online_training_args = TrainingArguments(\n",
    "            output_dir=\"./mistral-finetuned\",\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=1,\n",
    "            per_device_train_batch_size=1,\n",
    "            logging_dir='./logs_online',\n",
    "            logging_steps=10,\n",
    "            learning_rate=1e-5,\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.mistral_model,\n",
    "            args=online_training_args,\n",
    "            train_dataset=feedback_dataset,\n",
    "            data_collator=DataCollatorForLanguageModeling(tokenizer=self.mistral_tokenizer, mlm=False)\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        self.mistral_model.save_pretrained(\"./mistral-finetuned\")\n",
    "        self.mistral_tokenizer.save_pretrained(\"./mistral-finetuned\")\n",
    "        self.feedback_buffer = []\n",
    "        print(\"Model updated with new feedback.\")\n",
    "\n",
    "# Initialize chatbot\n",
    "chatbot = MentalHealthChatbot(bert_model, bert_tokenizer, mistral_model, mistral_tokenizer)\n",
    "\n",
    "# Evaluation functions\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    return math.exp(loss.item())\n",
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference.split()], hypothesis.split(), smoothing_function=smoothie)\n",
    "\n",
    "def calculate_rouge(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    return scorer.score(reference, hypothesis)\n",
    "\n",
    "def distinct_n_gram(responses, n=2):\n",
    "    all_ngrams = []\n",
    "    for response in responses:\n",
    "        tokens = response.split()\n",
    "        ngrams = list(zip(*[tokens[i:] for i in range(n)]))\n",
    "        all_ngrams.extend(ngrams)\n",
    "    total = len(all_ngrams)\n",
    "    unique = len(set(all_ngrams))\n",
    "    return unique / total if total > 0 else 0\n",
    "\n",
    "def measure_inference_time(generated_text, prompt, tokenizer):\n",
    "    return len(generated_text.split()) / 50.0\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    cpu_mem = process.memory_info().rss / 1024 ** 2\n",
    "    gpu_mem = torch.cuda.max_memory_allocated() / 1024 ** 2 if torch.cuda.is_available() else None\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "    gc.collect()\n",
    "    return {'cpu_memory_MB': cpu_mem, 'gpu_memory_MB': gpu_mem}\n",
    "\n",
    "# Evaluation using a subset of val_dataset\n",
    "eval_subset = val_dataset.select(range(min(20, len(val_dataset))))\n",
    "input_prompts = [item[\"prompt\"] for item in eval_subset]\n",
    "reference_responses = [item[\"response\"] for item in eval_subset]\n",
    "reference_emotions = [item[\"emotion\"] for item in eval_subset]\n",
    "generated_responses = []\n",
    "predicted_emotions = []\n",
    "\n",
    "# Generate responses\n",
    "mistral_model.eval()\n",
    "for prompt in tqdm(input_prompts, desc=\"Generating responses\"):\n",
    "    response = chatbot.generate_response(prompt.replace(\"Client: \", \"\"))\n",
    "    generated_responses.append(response[\"response\"])\n",
    "    predicted_emotions.append(response[\"emotion_detected\"])\n",
    "\n",
    "# Initialize accumulators\n",
    "bleu_scores = []\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "perplexities = []\n",
    "inference_times = []\n",
    "memory_usages = []\n",
    "emotion_correct = 0\n",
    "\n",
    "for ref, gen, prompt, ref_emotion, pred_emotion in zip(reference_responses, generated_responses, input_prompts, reference_emotions, predicted_emotions):\n",
    "    bleu_scores.append(calculate_bleu(ref, gen))\n",
    "    rouge = calculate_rouge(ref, gen)\n",
    "    for key in rouge_scores:\n",
    "        rouge_scores[key].append(rouge[key].fmeasure)\n",
    "    perplexities.append(calculate_perplexity(mistral_model, mistral_tokenizer, gen))\n",
    "    inference_times.append(measure_inference_time(gen, prompt, mistral_tokenizer))\n",
    "    memory_usages.append(get_memory_usage())\n",
    "    if ref_emotion != 'unknown' and ref_emotion.lower() == pred_emotion.lower():\n",
    "        emotion_correct += 1\n",
    "\n",
    "# Compute distinct-n and emotion accuracy\n",
    "distinct_1 = distinct_n_gram(generated_responses, n=1)\n",
    "distinct_2 = distinct_n_gram(generated_responses, n=2)\n",
    "emotion_accuracy = emotion_correct / sum(1 for e in reference_emotions if e != 'unknown') if sum(1 for e in reference_emotions if e != 'unknown') > 0 else 0\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\n--- Evaluation Metrics ---\")\n",
    "print(f\"Avg BLEU Score: {sum(bleu_scores)/len(bleu_scores):.4f}\")\n",
    "print(f\"Avg ROUGE-1: {sum(rouge_scores['rouge1'])/len(rouge_scores['rouge1']):.4f}\")\n",
    "print(f\"Avg ROUGE-2: {sum(rouge_scores['rouge2'])/len(rouge_scores['rouge2']):.4f}\")\n",
    "print(f\"Avg ROUGE-L: {sum(rouge_scores['rougeL'])/len(rouge_scores['rougeL']):.4f}\")\n",
    "print(f\"Avg Perplexity: {sum(perplexities)/len(perplexities):.4f}\")\n",
    "print(f\"Distinct-1: {distinct_1:.4f}\")\n",
    "print(f\"Distinct-2: {distinct_2:.4f}\")\n",
    "print(f\"Avg Inference Time: {sum(inference_times)/len(inference_times):.4f} seconds\")\n",
    "print(f\"Avg CPU Memory: {sum([m['cpu_memory_MB'] for m in memory_usages])/len(memory_usages):.2f} MB\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Avg GPU Memory: {sum([m['gpu_memory_MB'] for m in memory_usages if m['gpu_memory_MB'] is not None])/len(memory_usages):.2f} MB\")\n",
    "print(f\"Emotion Detection Accuracy: {emotion_accuracy:.4f}\")\n",
    "\n",
    "# Streamlit UI\n",
    "def run_streamlit():\n",
    "    st.title(\"Mental Health Chatbot\")\n",
    "    st.write(\"Talk to your virtual therapist. Rate responses to help me improve.\")\n",
    "\n",
    "    # User profile setup\n",
    "    if 'profile_set' not in st.session_state:\n",
    "        st.session_state.profile_set = False\n",
    "    if not st.session_state.profile_set:\n",
    "        tone = st.selectbox(\"Preferred response tone:\", [\"Neutral\", \"Concise\", \"Supportive\"])\n",
    "        if st.button(\"Set Profile\"):\n",
    "            chatbot.set_user_profile(tone.lower())\n",
    "            st.session_state.profile_set = True\n",
    "            st.success(f\"Profile set to {tone.lower()} tone.\")\n",
    "        return\n",
    "\n",
    "    # Chat interface\n",
    "    user_input = st.text_input(\"Your message:\", key=\"user_input\")\n",
    "    if st.button(\"Send\") and user_input.strip():\n",
    "        response = chatbot.generate_response(user_input)\n",
    "        st.write(f\"**Therapist**: {response['response']}\")\n",
    "        st.write(f\"**Detected Emotion**: {response['emotion_detected']} (Confidence: {response['confidence']:.2f})\")\n",
    "\n",
    "        # Feedback\n",
    "        feedback = st.radio(\"Was this response helpful?\", (\"Yes (1)\", \"No (0)\"), key=f\"feedback_{user_input}\")\n",
    "        preferred_response = \"\"\n",
    "        if feedback == \"No (0)\":\n",
    "            preferred_response = st.text_input(\"What would have been a better response? (Optional)\", key=f\"preferred_{user_input}\")\n",
    "\n",
    "        if st.button(\"Submit Feedback\", key=f\"submit_{user_input}\"):\n",
    "            # Validate feedback\n",
    "            if len(chatbot.feedback_buffer) >= 3 and all(fb['rating'] == (1 if feedback == \"Yes (1)\" else 0) for fb in chatbot.feedback_buffer[-3:]):\n",
    "                st.warning(\"Please provide varied feedback to help me improve.\")\n",
    "                return\n",
    "\n",
    "            rating = 1 if feedback == \"Yes (1)\" else 0\n",
    "            response_to_store = f\"Therapist: {preferred_response}\" if preferred_response.strip() and rating == 0 else f\"Therapist: {response['response']}\"\n",
    "            rating_to_store = 1 if preferred_response.strip() else rating\n",
    "\n",
    "            chatbot.feedback_buffer.append({\n",
    "                \"prompt\": f\"User tone: {chatbot.user_profile['tone']}\\nEmotion: {response['emotion_detected']}\\nClient: {user_input}\",\n",
    "                \"response\": response_to_store,\n",
    "                \"rating\": rating_to_store\n",
    "            })\n",
    "            chatbot.update_model_with_feedback()\n",
    "            st.success(\"Feedback recorded.\")\n",
    "\n",
    "# Save Streamlit app to a file\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "import streamlit as st\n",
    "from main import chatbot, run_streamlit\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_streamlit()\n",
    "\"\"\")\n",
    "\n",
    "# Run Streamlit\n",
    "public_url = ngrok.connect(8501)\n",
    "print(f\"Streamlit app running at: {public_url}\")\n",
    "!streamlit run app.py --server.port 8501\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}