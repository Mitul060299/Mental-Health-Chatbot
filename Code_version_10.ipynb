{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "# Mental Health Chatbot with Phi-3 Mini\n",
        "This notebook implements a chatbot fine-tuned on mental health dialogue datasets using Phi-3 Mini (3.8B). It includes data loading, model training, sentiment analysis, and a Streamlit app for interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup"
      },
      "outputs": [],
      "source": [
        "# Clean corrupted transformers and pyarrow installations\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/~ransformers\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/transformers\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/pyarrow\n",
        "!pip uninstall -y transformers sentence-transformers torch torchvision torchaudio datasets gcsfs fsspec pyarrow\n",
        "\n",
        "# Clear pip cache to avoid reusing corrupted packages\n",
        "!pip cache purge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Install dependencies with compatible versions\n",
        "%pip install --no-cache-dir --force-reinstall \\\n",
        "    torch==2.6.0 \\\n",
        "    torchvision==0.21.0 \\\n",
        "    torchaudio==2.6.0 \\\n",
        "    transformers==4.44.2 \\\n",
        "    datasets==3.6.0 \\\n",
        "    sentence-transformers==4.1.0 \\\n",
        "    pandas==2.2.3 \\\n",
        "    numpy==1.26.4 \\\n",
        "    tqdm==4.66.5 \\\n",
        "    nltk==3.9.1 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    bitsandbytes==0.43.3 \\\n",
        "    peft==0.9.0 \\\n",
        "    triton==2.3.1 \\\n",
        "    accelerate==1.0.1 \\\n",
        "    streamlit==1.45.1 \\\n",
        "    pyngrok==7.2.0 \\\n",
        "    serpapi==0.1.5 \\\n",
        "    fsspec==2025.3.0 \\\n",
        "    gcsfs==2025.3.0 \\\n",
        "    pyarrow==17.0.0 -q\n",
        "\n",
        "# Import rouge_scorer\n",
        "from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_cuda"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"CUDA is not available. Please select T4 GPU runtime.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "verify_versions"
      },
      "outputs": [],
      "source": [
        "# Verify installed versions\n",
        "import torch\n",
        "import transformers\n",
        "import datasets\n",
        "import fsspec\n",
        "import gcsfs\n",
        "import sentence_transformers\n",
        "import bitsandbytes\n",
        "import peft\n",
        "import pyarrow\n",
        "import triton\n",
        "print(f\"torch: {torch.__version__}\")\n",
        "print(f\"transformers: {transformers.__version__}\")\n",
        "print(f\"datasets: {datasets.__version__}\")\n",
        "print(f\"fsspec: {fsspec.__version__}\")\n",
        "print(f\"gcsfs: {gcsfs.__version__}\")\n",
        "print(f\"sentence-transformers: {sentence_transformers.__version__}\")\n",
        "print(f\"bitsandbytes: {bitsandbytes.__version__}\")\n",
        "print(f\"peft: {peft.__version__}\")\n",
        "print(f\"pyarrow: {pyarrow.__version__}\")\n",
        "print(f\"triton: {triton.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libraries"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import nltk\n",
        "from datasets import Dataset, load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BertTokenizer,\n",
        "    AutoModelForSequenceClassification\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from tqdm import tqdm\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_hf_token"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    if not HF_TOKEN:\n",
        "        raise ValueError(\"HF_TOKEN not found in Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading HF_TOKEN: {e}\")\n",
        "    print(\"Please set HF_TOKEN in Colab Secrets or input manually.\")\n",
        "    HF_TOKEN = input(\"Enter your Hugging Face token: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_datasets"
      },
      "outputs": [],
      "source": [
        "def load_and_merge_datasets():\n",
        "    dialogue_data = []\n",
        "    \n",
        "    # Load HOPE dataset\n",
        "    try:\n",
        "        with open('/content/drive/MyDrive/hope.json', 'r') as f:\n",
        "            hope_data = json.load(f)\n",
        "        hope_pairs = [\n",
        "            {\n",
        "                \"prompt\": f\"Client: {item['client']}\",\n",
        "                \"response\": f\"Therapist: {item['therapist']}\",\n",
        "                \"emotion\": item.get('emotion', 'unknown'),\n",
        "                \"source\": \"HOPE\"\n",
        "            }\n",
        "            for item in hope_data\n",
        "        ]\n",
        "        dialogue_data.extend(hope_pairs)\n",
        "        print(f\"Loaded {len(hope_pairs)} pairs from HOPE\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading HOPE: {e}\")\n",
        "    \n",
        "    # Load EmpatheticDialogues\n",
        "    try:\n",
        "        empathetic_df = pd.read_csv('/content/drive/MyDrive/empatheticdialogues.csv')\n",
        "        empathy_pairs = [\n",
        "            {\n",
        "                \"prompt\": f\"Client: {row['prompt']}\",\n",
        "                \"response\": f\"Therapist: {row['response']}\",\n",
        "                \"emotion\": row.get('emotion', 'unknown'),\n",
        "                \"source\": \"EmpatheticDialogues\"\n",
        "            }\n",
        "            for _, row in empathetic_df.iterrows()\n",
        "        ]\n",
        "        dialogue_data.extend(empathy_pairs)\n",
        "        print(f\"Loaded {len(empathy_pairs)} pairs from EmpatheticDialogues\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading EmpatheticDialogues: {e}\")\n",
        "    \n",
        "    # Load CounselChat\n",
        "    try:\n",
        "        counsel_df = pd.read_csv('/content/drive/MyDrive/counselchat.csv')\n",
        "        counsel_pairs = [\n",
        "            {\n",
        "                \"prompt\": f\"Client: {row['questionText']}\",\n",
        "                \"response\": f\"Therapist: {row['answerText']}\",\n",
        "                \"emotion\": row.get('emotion', 'unknown'),\n",
        "                \"source\": \"CounselChat\"\n",
        "            }\n",
        "            for _, row in counsel_df.iterrows()\n",
        "        ]\n",
        "        dialogue_data.extend(counsel_pairs)\n",
        "        print(f\"Loaded {len(counsel_pairs)} pairs from CounselChat\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CounselChat: {e}\")\n",
        "    \n",
        "    # Add new dataset (placeholder for continuous development)\n",
        "    try:\n",
        "        new_df = pd.read_csv('/content/drive/MyDrive/new_therapy_data.csv')\n",
        "        new_pairs = [\n",
        "            {\n",
        "                \"prompt\": f\"Client: {row['client_text']}\",\n",
        "                \"response\": f\"Therapist: {row['therapist_text']}\",\n",
        "                \"emotion\": row.get('emotion', 'unknown'),\n",
        "                \"source\": \"NewDataset\"\n",
        "            }\n",
        "            for _, row in new_df.iterrows()\n",
        "        ]\n",
        "        dialogue_data.extend(new_pairs)\n",
        "        print(f\"Added {len(new_pairs)} pairs from NewDataset\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading new dataset: {e}\")\n",
        "    \n",
        "    # Convert to Dataset\n",
        "    dialogue_data = [{'prompt': d['prompt'], 'response': d['response'], 'emotion': d.get('emotion', 'unknown')} for d in dialogue_data]\n",
        "    return Dataset.from_list(dialogue_data)\n",
        "\n",
        "# Load and split dataset\n",
        "dataset = load_and_merge_datasets()\n",
        "train_data, val_data = train_test_split(dataset.to_pandas(), test_size=0.2, random_state=42)\n",
        "train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
        "val_dataset = Dataset.from_pandas(val_data.reset_index(drop=True))\n",
        "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_models"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/phi-finetuned\"\n",
        "if os.path.exists(model_path) and os.path.isfile(os.path.join(model_path, \"config.json\")):\n",
        "    try:\n",
        "        phi_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        phi_tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_path,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(f\"Loaded fine-tuned Phi-3 Mini from {model_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading fine-tuned model: {e}\")\n",
        "        phi_model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        phi_tokenizer = AutoTokenizer.from_pretrained(\n",
        "            \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "else:\n",
        "    print(f\"Fine-tuned model not found at {model_path}. Loading pre-trained Phi-3 Mini.\")\n",
        "    phi_model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    phi_tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "phi_tokenizer.pad_token = phi_tokenizer.eos_token\n",
        "\n",
        "# Load BERT for sentiment analysis\n",
        "try:\n",
        "    bert_model = AutoModelForSequenceClassification.from_pretrained(\"bhadresh-savani/bert-base-uncased-emotion\", token=HF_TOKEN)\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(\"bhadresh-savani/bert-base-uncased-emotion\", token=HF_TOKEN)\n",
        "    print(\"Loaded BERT model for sentiment analysis.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading BERT model: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fine_tune"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    texts = [prompt + \" \" + response for prompt, response in zip(examples['prompt'], examples['response'])]\n",
        "    tokenized = phi_tokenizer(texts, truncation=True, max_length=128, padding='max_length')\n",
        "    # Ensure labels are set to input_ids for causal LM\n",
        "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
        "    return tokenized\n",
        "\n",
        "# Limit dataset for memory\n",
        "train_dataset = train_dataset.select(range(min(10000, len(train_dataset))))\n",
        "val_dataset = val_dataset.select(range(min(2500, len(val_dataset))))\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove extra columns\n",
        "train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if col not in ['input_ids', 'attention_mask', 'labels']])\n",
        "val_dataset = val_dataset.remove_columns([col for col in val_dataset.column_names if col not in ['input_ids', 'attention_mask', 'labels']])\n",
        "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Debug dataset\n",
        "print(\"Train dataset columns:\", train_dataset.column_names)\n",
        "print(\"Sample train item:\", train_dataset[0].keys())\n",
        "\n",
        "# Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"qkv_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "phi_model = get_peft_model(phi_model, lora_config)\n",
        "\n",
        "# Ensure LoRA parameters require gradients\n",
        "phi_model.train()\n",
        "for name, param in phi_model.named_parameters():\n",
        "    if \"lora\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Disable cache for gradient checkpointing\n",
        "phi_model.config.use_cache = False\n",
        "\n",
        "# Debug: Test forward pass with a single batch\n",
        "print(\"Testing model forward pass...\")\n",
        "sample_batch = train_dataset[:1]\n",
        "input_ids = sample_batch['input_ids'].to('cuda')\n",
        "attention_mask = sample_batch['attention_mask'].to('cuda')\n",
        "labels = sample_batch['labels'].to('cuda')\n",
        "try:\n",
        "    outputs = phi_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    print(\"Forward pass successful. Loss:\", outputs.loss.item())\n",
        "except Exception as e:\n",
        "    print(f\"Forward pass failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# Manual training loop\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=phi_tokenizer, mlm=False)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(phi_model.parameters(), lr=2e-5)\n",
        "num_epochs = 3\n",
        "gradient_accumulation_steps = 4\n",
        "max_grad_norm = 0.5\n",
        "device = torch.device('cuda')\n",
        "phi_model.to(device)\n",
        "\n",
        "# Debug first batch\n",
        "print(\"Inspecting first batch from DataLoader...\")\n",
        "first_batch = next(iter(train_dataloader))\n",
        "print(\"Batch keys:\", first_batch.keys())\n",
        "print(\"Batch shapes:\", {k: v.shape for k, v in first_batch.items()})\n",
        "\n",
        "# Training loop\n",
        "phi_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")):\n",
        "        # Filter batch to only expected keys\n",
        "        valid_batch = {k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask', 'labels']}\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = phi_model(**valid_batch)\n",
        "        loss = outputs.loss / gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "        total_loss += loss.item() * gradient_accumulation_steps\n",
        "        \n",
        "        # Gradient accumulation\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(phi_model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        # Log every 100 steps\n",
        "        if (step + 1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Step {step+1}, Loss: {total_loss / (step + 1):.4f}\")\n",
        "    \n",
        "    print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(train_dataloader):.4f}\")\n",
        "\n",
        "# Save model\n",
        "phi_model.save_pretrained(\"/content/drive/MyDrive/phi-finetuned\")\n",
        "phi_tokenizer.save_pretrained(\"/content/drive/MyDrive/phi-finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chatbot_class"
      },
      "outputs": [],
      "source": [
        "class MentalHealthChatbot:\n",
        "    def __init__(self, model, tokenizer, bert_model, bert_tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.bert_model = bert_model\n",
        "        self.bert_tokenizer = bert_tokenizer\n",
        "        self.enable_sentiment = True\n",
        "        self.emotion_labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        try:\n",
        "            inputs = self.bert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert_model(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            emotion_idx = probs.argmax().item()\n",
        "            return self.emotion_labels[emotion_idx], probs[0][emotion_idx].item()\n",
        "        except Exception as e:\n",
        "            print(f\"Error in sentiment analysis: {e}\")\n",
        "            return \"unknown\", 0.0\n",
        "\n",
        "    def generate_response(self, user_input, max_length=100):\n",
        "        try:\n",
        "            prompt = f\"Client: {user_input}\"\n",
        "            if self.enable_sentiment:\n",
        "                emotion, confidence = self.analyze_sentiment(user_input)\n",
        "                prompt += f\" [Emotion: {emotion}, Confidence: {confidence:.2f}]\"\n",
        "            prompt += \" Therapist:\"\n",
        "\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=self.tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            therapist_response = response.split(\"Therapist:\")[-1].strip()\n",
        "            return therapist_response\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response: {e}\")\n",
        "            return \"I'm sorry, I couldn't process that. Can you try again?\"\n",
        "\n",
        "# Initialize chatbot\n",
        "chatbot = MentalHealthChatbot(phi_model, phi_tokenizer, bert_model, bert_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "streamlit_app"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, BertTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load models\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"/content/drive/MyDrive/phi-finetuned\",\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            \"/content/drive/MyDrive/phi-finetuned\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            \"bhadresh-savani/bert-base-uncased-emotion\"\n",
        "        )\n",
        "        bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            \"bhadresh-savani/bert-base-uncased-emotion\"\n",
        "        )\n",
        "        return model, tokenizer, bert_model, bert_tokenizer\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading models: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "model, tokenizer, bert_model, bert_tokenizer = load_models()\n",
        "\n",
        "# Chatbot class\n",
        "class MentalHealthChatbot:\n",
        "    def __init__(self, model, tokenizer, bert_model, bert_tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.bert_model = bert_model\n",
        "        self.bert_tokenizer = bert_tokenizer\n",
        "        self.enable_sentiment = True\n",
        "        self.emotion_labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        try:\n",
        "            inputs = self.bert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert_model(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            emotion_idx = probs.argmax().item()\n",
        "            return self.emotion_labels[emotion_idx], probs[0][emotion_idx].item()\n",
        "        except Exception as e:\n",
        "            return \"unknown\", 0.0\n",
        "\n",
        "    def generate_response(self, user_input, max_length=100):\n",
        "        try:\n",
        "            prompt = f\"Client: {user_input}\"\n",
        "            if self.enable_sentiment:\n",
        "                emotion, confidence = self.analyze_sentiment(user_input)\n",
        "                prompt += f\" [Emotion: {emotion}, Confidence: {confidence:.2f}]\"\n",
        "            prompt += \" Therapist:\"\n",
        "\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=1,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=self.tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            therapist_response = response.split(\"Therapist:\")[-1].strip()\n",
        "            return therapist_response\n",
        "        except Exception as e:\n",
        "            return \"I'm sorry, I couldn't process that. Can you try again?\"\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Mental Health Chatbot\")\n",
        "st.write(\"Talk to our AI therapist trained to provide empathetic responses.\")\n",
        "\n",
        "if model is None or tokenizer is None:\n",
        "    st.error(\"Failed to load models. Please check the notebook logs.\")\n",
        "else:\n",
        "    chatbot = MentalHealthChatbot(model, tokenizer, bert_model, bert_tokenizer)\n",
        "    \n",
        "    # Chat history\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "\n",
        "    # Display chat history\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "    # User input\n",
        "    if user_input := st.chat_input(\"How are you feeling today?\"):\n",
        "        # Add user message\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(user_input)\n",
        "\n",
        "        # Generate response\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                response = chatbot.generate_response(user_input)\n",
        "                st.markdown(response)\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_streamlit"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set ngrok authtoken\n",
        "ngrok.set_auth_token(\"YOUR_NGROK_AUTH_TOKEN\")  # Replace with your ngrok token\n",
        "\n",
        "# Start Streamlit server\n",
        "!streamlit run app.py --server.port 8501 &\n",
        "\n",
        "# Create ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app running at: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_chatbot"
      },
      "outputs": [],
      "source": [
        "test_input = \"I'm feeling really anxious today.\"\n",
        "response = chatbot.generate_response(test_input)\n",
        "print(f\"User: {test_input}\")\n",
        "print(f\"Therapist: {response}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8s2y2z3x4y5z6a7b8c9d0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}