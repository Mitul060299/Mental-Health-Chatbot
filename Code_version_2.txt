{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import psutil\n",
    "import gc\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Check if rouge_score is installed\n",
    "if importlib.util.find_spec(\"rouge_score\") is None:\n",
    "    print(\"Error: 'rouge_score' module not found. Please install it manually in a virtual environment or using pipx:\")\n",
    "    print(\"1. Create a virtual environment: python3 -m venv venv\")\n",
    "    print(\"2. Activate it: source venv/bin/activate\")\n",
    "    print(\"3. Install: pip install rouge_score\")\n",
    "    print(\"Or use pipx: pipx install rouge_score\")\n",
    "    sys.exit(1)\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, pipeline\n",
    "\n",
    "# Load and merge datasets\n",
    "def load_and_merge_datasets():\n",
    "    # STEP 1: Load HOPE Therapy Data\n",
    "    hope_path = \"/content/SPARTA_WSDM2022/HOPE_data/HOPE_therapy_session_transcripts\"\n",
    "    try:\n",
    "        files = [f for f in os.listdir(hope_path) if f.endswith(\".csv\")]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: HOPE dataset directory not found at {hope_path}\")\n",
    "        files = []\n",
    "\n",
    "    hope_pairs = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(hope_path, file))\n",
    "        # Map the Type column to proper roles\n",
    "        df['Speaker'] = df['Type'].map({'T': 'Therapist', 'P': 'Client'})\n",
    "        df['Content'] = df['Utterance']  # Rename for consistency\n",
    "\n",
    "        # Extract consecutive client-therapist exchanges\n",
    "        for i in range(1, len(df)):\n",
    "            if df.loc[i-1, 'Speaker'] == \"Client\" and df.loc[i, 'Speaker'] == \"Therapist\":\n",
    "                hope_pairs.append({\n",
    "                    \"prompt\": f\"Client: {df.loc[i-1, 'Content']}\",\n",
    "                    \"response\": f\"Therapist: {df.loc[i, 'Content']}\",\n",
    "                    \"source\": \"HOPE\"\n",
    "                })\n",
    "\n",
    "    print(f\"Extracted {len(hope_pairs)} dialogue pairs from HOPE dataset\")\n",
    "\n",
    "    # STEP 2: Load EmpatheticDialogues\n",
    "    try:\n",
    "        empathetic_ds = load_dataset(\"empathetic_dialogues\")\n",
    "        empathy_pairs = []\n",
    "\n",
    "        # Process conversations to get contextual exchanges\n",
    "        prev_conv_id = None\n",
    "        context = \"\"\n",
    "\n",
    "        for row in empathetic_ds['train']:\n",
    "            if row['utterance_idx'] > 0 and row['conv_id'] == prev_conv_id:\n",
    "                empathy_pairs.append({\n",
    "                    \"prompt\": f\"Client: {context}\",\n",
    "                    \"response\": f\"Therapist: {row['utterance']}\",\n",
    "                    \"emotion\": row['context'],\n",
    "                    \"source\": \"EmpatheticDialogues\"\n",
    "                })\n",
    "\n",
    "            context = row['utterance']\n",
    "            prev_conv_id = row['conv_id']\n",
    "\n",
    "        print(f\"Extracted {len(empathy_pairs)} dialogue pairs from EmpatheticDialogues dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading EmpatheticDialogues: {e}\")\n",
    "        empathy_pairs = []\n",
    "\n",
    "    # STEP 3: Load CounselChat\n",
    "    try:\n",
    "        # Download CounselChat data\n",
    "        url = \"https://raw.githubusercontent.com/nbertagnolli/counsel-chat/master/data/counselchat-data.csv\"\n",
    "        response = requests.get(url)\n",
    "        with open(\"counselchat-data.csv\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        cc_df = pd.read_csv(\"counselchat-data.csv\")\n",
    "        counsel_pairs = []\n",
    "\n",
    "        for _, row in cc_df.iterrows():\n",
    "            if pd.notnull(row['questionText']) and pd.notnull(row['answerText']):\n",
    "                counsel_pairs.append({\n",
    "                    \"prompt\": f\"Client: {row['questionText']}\",\n",
    "                    \"response\": f\"Therapist: {row['answerText'].replace('<p>','')}\",\n",
    "                    \"source\": \"CounselChat\"\n",
    "                })\n",
    "\n",
    "        print(f\"Extracted {len(counsel_pairs)} dialogue pairs from CounselChat dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CounselChat: {e}\")\n",
    "        counsel_pairs = []\n",
    "\n",
    "    # STEP 4: Merge All Dialogues\n",
    "    dialogue_data = hope_pairs + empathy_pairs + counsel_pairs\n",
    "    print(f\"Total dialogue pairs: {len(dialogue_data)}\")\n",
    "\n",
    "    # Optional: Sample checking of data quality\n",
    "    print(\"\\nSample data from each source:\")\n",
    "    for source in [\"HOPE\", \"EmpatheticDialogues\", \"CounselChat\"]:\n",
    "        samples = [d for d in dialogue_data if d.get(\"source\") == source]\n",
    "        if samples:\n",
    "            print(f\"\\n{source} sample:\")\n",
    "            sample = np.random.choice(samples)\n",
    "            print(f\"Prompt: {sample['prompt']}\")\n",
    "            print(f\"Response: {sample['response']}\")\n",
    "\n",
    "    # Convert to Hugging Face Dataset (excluding 'source' and 'emotion' for consistency)\n",
    "    dialogue_data = [{'prompt': d['prompt'], 'response': d['response']} for d in dialogue_data]\n",
    "    return Dataset.from_list(dialogue_data)\n",
    "\n",
    "# Load and split dataset\n",
    "dialogue_data = load_and_merge_datasets()\n",
    "train_data, val_data = train_test_split(dialogue_data.to_pandas(), test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_data.reset_index(drop=True))\n",
    "\n",
    "# Define MentalHealthChatbot class\n",
    "class MentalHealthChatbot:\n",
    "    def __init__(self, bert_model, bert_tokenizer, gpt_model, gpt_tokenizer):\n",
    "        self.bert_model = bert_model\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.gpt_model = gpt_model\n",
    "        self.gpt_tokenizer = gpt_tokenizer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Move models to correct device\n",
    "        self.bert_model.to(self.device)\n",
    "        self.gpt_model.to(self.device)\n",
    "\n",
    "        # Set models to evaluation mode\n",
    "        self.bert_model.eval()\n",
    "        self.gpt_model.eval()\n",
    "\n",
    "    def analyze_emotion(self, text):\n",
    "        \"\"\"Use BERT to analyze the emotional content and context of user input\"\"\"\n",
    "        inputs = self.bert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(**inputs)\n",
    "            emotion_scores = outputs.logits[0]\n",
    "            emotion_id = torch.argmax(emotion_scores).item()\n",
    "            emotion = self.bert_model.config.id2label.get(emotion_id, \"neutral\")\n",
    "            confidence = torch.softmax(emotion_scores, dim=0)[emotion_id].item()\n",
    "\n",
    "        return emotion, confidence\n",
    "\n",
    "    def generate_response(self, user_input, max_length=150):\n",
    "        \"\"\"Generate therapeutic response using BERT emotion analysis and GPT-2\"\"\"\n",
    "        emotion, confidence = self.analyze_emotion(user_input)\n",
    "        enhanced_prompt = f\"Client (feeling {emotion}): {user_input}\\nTherapist:\"\n",
    "        inputs = self.gpt_tokenizer(enhanced_prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        output_sequences = self.gpt_model.generate(\n",
    "            **inputs,\n",
    "            max_length=len(inputs[\"input_ids\"][0]) + max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.gpt_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        full_text = self.gpt_tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "        try:\n",
    "            therapist_response = full_text.split(\"Therapist:\", 1)[1].strip()\n",
    "        except IndexError:\n",
    "            therapist_response = full_text\n",
    "\n",
    "        response_data = {\n",
    "            \"response\": therapist_response,\n",
    "            \"emotion_detected\": emotion,\n",
    "            \"confidence\": confidence\n",
    "        }\n",
    "        return response_data\n",
    "\n",
    "# Load models (using pre-trained for demonstration; replace with fine-tuned models after training)\n",
    "try:\n",
    "    bert_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "    gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "except FileNotFoundError:\n",
    "    print(\"Fine-tuned models not found. Using pre-trained models. Please train models for better results.\")\n",
    "    bert_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "    gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "# Initialize chatbot\n",
    "chatbot = MentalHealthChatbot(bert_model, bert_tokenizer, gpt_model, gpt_tokenizer)\n",
    "\n",
    "# Test response generation\n",
    "sample_input = \"I've been feeling really down lately and nothing seems to help.\"\n",
    "response = chatbot.generate_response(sample_input)\n",
    "print(f\"Sample Input: {sample_input}\")\n",
    "print(f\"Detected emotion: {response['emotion_detected']} (confidence: {response['confidence']:.2f})\")\n",
    "print(f\"Response: {response['response']}\")\n",
    "\n",
    "# Evaluation functions\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    return math.exp(loss.item())\n",
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference.split()], hypothesis.split(), smoothing_function=smoothie)\n",
    "\n",
    "def calculate_rouge(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    return scorer.score(reference, hypothesis)\n",
    "\n",
    "def distinct_n_gram(responses, n=2):\n",
    "    all_ngrams = []\n",
    "    for response in responses:\n",
    "        tokens = response.split()\n",
    "        ngrams = list(zip(*[tokens[i:] for i in range(n)]))\n",
    "        all_ngrams.extend(ngrams)\n",
    "    total = len(all_ngrams)\n",
    "    unique = len(set(all_ngrams))\n",
    "    return unique / total if total > 0 else 0\n",
    "\n",
    "def measure_inference_time(model, tokenizer, prompt):\n",
    "    start = time.time()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    _ = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    cpu_mem = process.memory_info().rss / 1024 ** 2  # in MB\n",
    "    gpu_mem = torch.cuda.max_memory_allocated() / 1024 ** 2 if torch.cuda.is_available() else None\n",
    "    torch.cuda.reset_max_memory_allocated() if torch.cuda.is_available() else None\n",
    "    gc.collect()\n",
    "    return {'cpu_memory_MB': cpu_mem, 'gpu_memory_MB': gpu_mem}\n",
    "\n",
    "# Evaluation using val_dataset\n",
    "input_prompts = [item[\"prompt\"] for item in val_dataset]\n",
    "reference_responses = [item[\"response\"] for item in val_dataset]\n",
    "generated_responses = []\n",
    "\n",
    "# Generate responses using chatbot\n",
    "gpt_model.eval()\n",
    "for prompt in tqdm(input_prompts, desc=\"Generating responses\"):\n",
    "    response = chatbot.generate_response(prompt)\n",
    "    generated_responses.append(response[\"response\"])\n",
    "\n",
    "# Initialize accumulators\n",
    "bleu_scores = []\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "perplexities = []\n",
    "inference_times = []\n",
    "memory_usages = []\n",
    "\n",
    "for ref, gen, prompt in zip(reference_responses, generated_responses, input_prompts):\n",
    "    bleu_scores.append(calculate_bleu(ref, gen))\n",
    "    rouge = calculate_rouge(ref, gen)\n",
    "    for key in rouge_scores:\n",
    "        rouge_scores[key].append(rouge[key].fmeasure)\n",
    "    perplexities.append(calculate_perplexity(gpt_model, gpt_tokenizer, gen))\n",
    "    inference_times.append(measure_inference_time(gpt_model, gpt_tokenizer, prompt))\n",
    "    memory_usages.append(get_memory_usage())\n",
    "\n",
    "# Compute distinct-n\n",
    "distinct_1 = distinct_n_gram(generated_responses, n=1)\n",
    "distinct_2 = distinct_n_gram(generated_responses, n=2)\n",
    "\n",
    "# Print average results\n",
    "print(\"\\n--- Evaluation Metrics ---\")\n",
    "print(f\"Avg BLEU Score: {sum(bleu_scores)/len(bleu_scores):.4f}\")\n",
    "print(f\"Avg ROUGE-1: {sum(rouge_scores['rouge1'])/len(rouge_scores['rouge1']):.4f}\")\n",
    "print(f\"Avg ROUGE-2: {sum(rouge_scores['rouge2'])/len(rouge_scores['rouge2']):.4f}\")\n",
    "print(f\"Avg ROUGE-L: {sum(rouge_scores['rougeL'])/len(rouge_scores['rougeL']):.4f}\")\n",
    "print(f\"Avg Perplexity: {sum(perplexities)/len(perplexities):.4f}\")\n",
    "print(f\"Distinct-1: {distinct_1:.4f}\")\n",
    "print(f\"Distinct-2: {distinct_2:.4f}\")\n",
    "print(f\"Avg Inference Time: {sum(inference_times)/len(inference_times):.4f} seconds\")\n",
    "print(f\"Avg CPU Memory: {sum([m['cpu_memory_MB'] for m in memory_usages])/len(memory_usages):.2f} MB\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Avg GPU Memory: {sum([m['gpu_memory_MB'] for m in memory_usages if m['gpu_memory_MB'] is not None])/len(memory_usages):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}