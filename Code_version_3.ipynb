{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ “cells”: \\[ { “cell_type”: “code”, “execution_count”: null,\n",
    "“metadata”: {}, “outputs”: \\[\\], “source”: \\[ “import importlib.util”,\n",
    "“import sys”, “import torch”, “import math”, “import os”, “import pandas\n",
    "as pd”, “import numpy as np”, “from tqdm import tqdm”, “from\n",
    "nltk.translate.bleu_score import sentence_bleu, SmoothingFunction”,\n",
    "“from datasets import load_dataset, Dataset, concatenate_datasets”,\n",
    "“from sklearn.model_selection import train_test_split”, “import psutil”,\n",
    "“import gc”, “import time”, “import requests”, “”, “\\# Check if\n",
    "rouge_score is installed”, “if importlib.util.find_spec(\"rouge_score\")\n",
    "is None:”, ” print(\"Error: ‘rouge_score’ module not found. Please\n",
    "install it manually in a virtual environment or using pipx:\")“,”\n",
    "print(\"1. Create a virtual environment: python3 -m venv venv\")“,”\n",
    "print(\"2. Activate it: source venv/bin/activate\")“,” print(\"3. Install:\n",
    "pip install rouge_score\")“,” print(\"Or use pipx: pipx install\n",
    "rouge_score\")“,” sys.exit(1)“,”“,”from rouge_score import\n",
    "rouge_scorer“,”from transformers import AutoModelForCausalLM,\n",
    "AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer,\n",
    "Trainer, TrainingArguments, DataCollatorForLanguageModeling,\n",
    "pipeline“,”“,”\\# Load and merge datasets“,”def\n",
    "load_and_merge_datasets():“,” \\# STEP 1: Load HOPE Therapy Data“,”\n",
    "hope_path =\n",
    "\"/content/SPARTA_WSDM2022/HOPE_data/HOPE_therapy_session_transcripts\"“,”\n",
    "try:“,” files = \\[f for f in os.listdir(hope_path) if\n",
    "f.endswith(\".csv\")\\]“,” except FileNotFoundError:“,” print(f\"Error: HOPE\n",
    "dataset directory not found at {hope_path}\")“,” files = \\[\\]“,”“,”\n",
    "hope_pairs = \\[\\]“,” for file in files:“,” df =\n",
    "pd.read_csv(os.path.join(hope_path, file))“,” \\# Map the Type column to\n",
    "proper roles“,” df\\[‘Speaker’\\] = df\\[‘Type’\\].map({‘T’: ‘Therapist’,\n",
    "‘P’: ‘Client’})“,” df\\[‘Content’\\] = df\\[‘Utterance’\\] \\# Rename for\n",
    "consistency“,”“,” \\# Extract consecutive client-therapist exchanges“,”\n",
    "for i in range(1, len(df)):“,” if df.loc\\[i-1, ‘Speaker’\\] == \"Client\"\n",
    "and df.loc\\[i, ‘Speaker’\\] == \"Therapist\":“,” hope_pairs.append({“,”\n",
    "\"prompt\": f\"Client: {df.loc\\[i-1, ‘Content’\\]}\",“,” \"response\":\n",
    "f\"Therapist: {df.loc\\[i, ‘Content’\\]}\",“,” \"source\": \"HOPE\"“,” })“,”“,”\n",
    "print(f\"Extracted {len(hope_pairs)} dialogue pairs from HOPE\n",
    "dataset\")“,”“,” \\# STEP 2: Load EmpatheticDialogues“,” try:“,”\n",
    "empathetic_ds = load_dataset(\"empathetic_dialogues\")“,” empathy_pairs =\n",
    "\\[\\]“,”“,” \\# Process conversations to get contextual exchanges“,”\n",
    "prev_conv_id = None“,” context = \"\"“,”“,” for row in\n",
    "empathetic_ds\\[‘train’\\]:“,” if row\\[‘utterance_idx’\\] \\> 0 and\n",
    "row\\[‘conv_id’\\] == prev_conv_id:“,” empathy_pairs.append({“,” \"prompt\":\n",
    "f\"Client: {context}\",“,” \"response\": f\"Therapist:\n",
    "{row\\[‘utterance’\\]}\",“,” \"emotion\": row\\[‘context’\\],“,” \"source\":\n",
    "\"EmpatheticDialogues\"“,” })“,”“,” context = row\\[‘utterance’\\]“,”\n",
    "prev_conv_id = row\\[‘conv_id’\\]“,”“,” print(f\"Extracted\n",
    "{len(empathy_pairs)} dialogue pairs from EmpatheticDialogues\n",
    "dataset\")“,” except Exception as e:“,” print(f\"Error loading\n",
    "EmpatheticDialogues: {e}\")“,” empathy_pairs = \\[\\]“,”“,” \\# STEP 3: Load\n",
    "CounselChat“,” try:“,” \\# Download CounselChat data“,” url =\n",
    "\"https://raw.githubusercontent.com/nbertagnolli/counsel-chat/master/data/counselchat-data.csv\"“,”\n",
    "response = requests.get(url)“,” with open(\"counselchat-data.csv\", \"wb\")\n",
    "as f:“,” f.write(response.content)“,”“,” cc_df =\n",
    "pd.read_csv(\"counselchat-data.csv\")“,” counsel_pairs = \\[\\]“,”“,” for\n",
    "\\_, row in cc_df.iterrows():“,” if pd.notnull(row\\[‘questionText’\\]) and\n",
    "pd.notnull(row\\[‘answerText’\\]):“,” counsel_pairs.append({“,” \"prompt\":\n",
    "f\"Client: {row\\[‘questionText’\\]}\",“,” \"response\": f\"Therapist:\n",
    "{row\\[‘answerText’\\].replace(’\\] } \\], “metadata”: { “kernelspec”: {\n",
    "“display_name”: “Python 3”, “language”: “python”, “name”: “python3” },\n",
    "“language_info”: { “codemirror_mode”: { “name”: “ipython”, “version”: 3\n",
    "}, “file_extension”: “.py”, “mimetype”: “text/x-python”, “name”:\n",
    "“python”, “nbconvert_exporter”: “python”, “pygments_lexer”: “ipython3”,\n",
    "“version”: “3.12.0” } }, “nbformat”: 4, “nbformat_minor”: 5 }"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
